{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages and data_utils.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/samart/Desktop/LEAP/Climsim/ClimSim_Playground/preprocessing/..'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.join(os.getcwd()  , '..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/samart/Desktop/LEAP/Climsim/ClimSim_Playground/preprocessing',\n",
       " '/Users/samart/anaconda3/envs/leap/lib/python311.zip',\n",
       " '/Users/samart/anaconda3/envs/leap/lib/python3.11',\n",
       " '/Users/samart/anaconda3/envs/leap/lib/python3.11/lib-dynload',\n",
       " '',\n",
       " '/Users/samart/anaconda3/envs/leap/lib/python3.11/site-packages',\n",
       " '/Users/samart/anaconda3/envs/leap/lib/python3.11/site-packages/PyNIO']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:08:01.087221: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'grid_info' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgrid_info\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'grid_info' is not defined"
     ]
    }
   ],
   "source": [
    "grid_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = '../grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = 'normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:09:26.392976: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-10-18 01:09:26.393605: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2023-10-18 01:09:26.540750: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m data\u001b[38;5;241m.\u001b[39mset_filelist(data_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# save numpy files of training data\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/climsim_utils/data_utils.py:387\u001b[0m, in \u001b[0;36mdata_utils.save_as_npy\u001b[0;34m(self, data_split, save_path, save_latlontime_dict)\u001b[0m\n\u001b[1;32m    385\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ncdata_with_generator(data_split)\n\u001b[1;32m    386\u001b[0m npy_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mas_numpy_iterator())\n\u001b[0;32m--> 387\u001b[0m npy_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m npy_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([npy_iterator[x][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(npy_iterator))])\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_input.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# set data path\n",
    "data.data_path = 'Data/'\n",
    "\n",
    "# set inputs and outputs to V1 subset\n",
    "data.set_to_v1_vars()\n",
    "\n",
    "# v1 inputs (name :: description :: dimension :: units): \n",
    "\n",
    "# 'state_t' :: air temperature :: 60 :: K \n",
    "# 'state_q0001' :: specific humidity :: 60 :: kg/kg\n",
    "# 'state_ps' :: surface pressure :: 1 :: Pa\n",
    "# 'pbuf_SOLIN' :: solar insolation :: 1 :: W/m^2\n",
    "# 'pbuf_LHFLX' :: surface latent heat flux :: 1 :: W/m^2\n",
    "# 'pbuf_SHFLX' :: surface sensible heat flux :: 1 :: W/m^2\n",
    "\n",
    "# v1 outputs (name :: description :: dimension :: units): \n",
    "\n",
    "# 'ptend_t' :: heating tendency :: 60 :: K/s \n",
    "# 'ptend_q0001' :: moistening tendency :: 60 :: kg/kg/s\n",
    "# 'cam_out_NETSW' :: net shortwave flux at surface :: 1 :: W/m^2\n",
    "# 'cam_out_FLWDS' :: downward longwave flux at surface :: 1 :: W/m^2 \n",
    "# 'cam_out_PRECSC' :: snow rate (liquid water equivalent) :: 1 :: m/s \n",
    "# 'cam_out_PRECC' :: rain rate :: 1 :: m/s\n",
    "# 'cam_out_SOLS' :: downward visible direct solar flux to surface :: 1 :: W/m^2\n",
    "# 'cam_out_SOLL' :: downward near-infrared direct solar flux to surface :: 1 :: W/m^2\n",
    "# 'cam_out_SOLSD' :: downward diffuse solar flux to surface :: 1 :: W/m^2\n",
    "# 'cam_out_SOLLD' :: downward diffuse near-infrared solar flux to surface :: 1 :: W/m^2\n",
    "\n",
    "# set regular expressions for selecting training data\n",
    "data.set_regexps(data_split = 'train', \n",
    "                 regexps = ['E3SM-MMF.mli.000[1234567]-*-*-*.nc', # years 1 through 7\n",
    "                            'E3SM-MMF.mli.0008-01-*-*.nc']) # first month of year 8\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split = 'train', stride_sample = 7)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'train')\n",
    "# save numpy files of training data\n",
    "data.save_as_npy(data_split = 'train', save_path = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:09:50.640960: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39mset_filelist(data_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# save numpy files of validation data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/climsim_utils/data_utils.py:387\u001b[0m, in \u001b[0;36mdata_utils.save_as_npy\u001b[0;34m(self, data_split, save_path, save_latlontime_dict)\u001b[0m\n\u001b[1;32m    385\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ncdata_with_generator(data_split)\n\u001b[1;32m    386\u001b[0m npy_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mas_numpy_iterator())\n\u001b[0;32m--> 387\u001b[0m npy_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m npy_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([npy_iterator[x][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(npy_iterator))])\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_input.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# set regular expressions for selecting validation data\n",
    "data.set_regexps(data_split = 'val',\n",
    "                 regexps = ['E3SM-MMF.mli.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
    "                            'E3SM-MMF.mli.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
    "                            'E3SM-MMF.mli.0009-01-*-*.nc']) # first month of year 9\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split = 'val', stride_sample = 7)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'val')\n",
    "# save numpy files of validation data\n",
    "data.save_as_npy(data_split = 'val', save_path = '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create scoring data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 01:09:54.512936: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype int32\n",
      "\t [[{{node Placeholder/_0}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need at least one array to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39mset_filelist(data_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mscoring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# save numpy files of scoring data\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_as_npy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mscoring\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/climsim_utils/data_utils.py:387\u001b[0m, in \u001b[0;36mdata_utils.save_as_npy\u001b[0;34m(self, data_split, save_path, save_latlontime_dict)\u001b[0m\n\u001b[1;32m    385\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_ncdata_with_generator(data_split)\n\u001b[1;32m    386\u001b[0m npy_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data_loader\u001b[38;5;241m.\u001b[39mas_numpy_iterator())\n\u001b[0;32m--> 387\u001b[0m npy_input \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnpy_iterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    388\u001b[0m npy_target \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([npy_iterator[x][\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(npy_iterator))])\n\u001b[1;32m    389\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(save_path \u001b[38;5;241m+\u001b[39m prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_input.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"
     ]
    }
   ],
   "source": [
    "# set regular expressions for selecting scoring data (stride of 6 is needed for daily averaging)\n",
    "data.set_regexps(data_split = 'scoring',\n",
    "                 regexps = ['E3SM-MMF.mli.0008-0[23456789]-*-*.nc', # months 2 through 9 of year 8\n",
    "                            'E3SM-MMF.mli.0008-1[012]-*-*.nc', # months 10 through 12 of year 8\n",
    "                            'E3SM-MMF.mli.0009-01-*-*.nc']) # first month of year 9\n",
    "# set temporal subsampling\n",
    "data.set_stride_sample(data_split = 'scoring', stride_sample = 6)\n",
    "# create list of files to extract data from\n",
    "data.set_filelist(data_split = 'scoring')\n",
    "# save numpy files of scoring data\n",
    "data.save_as_npy(data_split = 'scoring', save_path = '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished\n"
     ]
    }
   ],
   "source": [
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
