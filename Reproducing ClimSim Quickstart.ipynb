{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64be0329",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-18 17:35:27.132587: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from climsim_utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a0d81ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eb12b7",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "Link: https://huggingface.co/datasets/LEAP/subsampled_low_res/tree/main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b94f0aa",
   "metadata": {},
   "source": [
    "### Loading using huggingface datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "267da370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/lc/mjy2_vc10zd7k8yny4sjghbr0000gn/T/ipykernel_6027/216168498.py:2: FutureWarning: list_datasets is deprecated and will be removed in the next major version of datasets. Use 'huggingface_hub.list_datasets' instead.\n",
      "  datasets_list = datasets.list_datasets()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70759\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "datasets_list = datasets.list_datasets()\n",
    "print(len(datasets_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4cec327a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['acronym_identification',\n",
       " 'ade_corpus_v2',\n",
       " 'adversarial_qa',\n",
       " 'aeslc',\n",
       " 'afrikaans_ner_corpus']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747013a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'LEAP/subsampled_low_res' in datasets_list ## But too expensive to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4707010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "ds_builder = datasets.load_dataset_builder('LEAP/subsampled_low_res')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77384869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParquetConfig(name='default', version=0.0.0, data_dir=None, data_files={NamedSplit('train'): ['hf://datasets/LEAP/subsampled_low_res@7ee4456a9716a9ae4a0ae2b53ec68ba036c2b229/train_input.parquet', 'hf://datasets/LEAP/subsampled_low_res@7ee4456a9716a9ae4a0ae2b53ec68ba036c2b229/train_target.parquet'], NamedSplit('validation'): ['hf://datasets/LEAP/subsampled_low_res@7ee4456a9716a9ae4a0ae2b53ec68ba036c2b229/val_input.parquet', 'hf://datasets/LEAP/subsampled_low_res@7ee4456a9716a9ae4a0ae2b53ec68ba036c2b229/val_target.parquet']}, description=None, batch_size=10000, columns=None, features=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_builder.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b976afaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99d544a6",
   "metadata": {},
   "source": [
    "### Downloading Files Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93b3f4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_path = 'ClimSim_Playground/grid_info/ClimSim_low-res_grid-info.nc'\n",
    "norm_path = 'ClimSim_Playground/preprocessing/normalizations/'\n",
    "\n",
    "grid_info = xr.open_dataset(grid_path)\n",
    "input_mean = xr.open_dataset(norm_path + 'inputs/input_mean.nc')\n",
    "input_max = xr.open_dataset(norm_path + 'inputs/input_max.nc')\n",
    "input_min = xr.open_dataset(norm_path + 'inputs/input_min.nc')\n",
    "output_scale = xr.open_dataset(norm_path + 'outputs/output_scale.nc')\n",
    "\n",
    "data = data_utils(grid_info = grid_info, \n",
    "                  input_mean = input_mean, \n",
    "                  input_max = input_max, \n",
    "                  input_min = input_min, \n",
    "                  output_scale = output_scale)\n",
    "\n",
    "# set variables to V1 subset\n",
    "data.set_to_v1_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "71d91b57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Data/val_input.npy'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_input_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a5dbe410",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "Failed to interpret file 'Data/val_input.npy' as a pickle",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/numpy/lib/npyio.py:441\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_input_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/numpy/lib/npyio.py:443\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 443\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to interpret file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m as a pickle\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Failed to interpret file 'Data/val_input.npy' as a pickle"
     ]
    }
   ],
   "source": [
    "np.load(val_input_path, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ca000bf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load file containing pickled data when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m data\u001b[38;5;241m.\u001b[39minput_train \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_npy_file(train_input_path)\n\u001b[1;32m     10\u001b[0m data\u001b[38;5;241m.\u001b[39mtarget_train \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_npy_file(train_target_path)\n\u001b[0;32m---> 11\u001b[0m data\u001b[38;5;241m.\u001b[39minput_val \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_npy_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_input_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m data\u001b[38;5;241m.\u001b[39mtarget_val \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mload_npy_file(val_target_path)\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/climsim_utils/data_utils.py:454\u001b[0m, in \u001b[0;36mdata_utils.load_npy_file\u001b[0;34m(load_path)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;124;03mThis function loads the prediction .npy file.\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(load_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m--> 454\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pred\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/numpy/lib/npyio.py:438\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 438\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load file containing pickled data \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    439\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen allow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mload(fid, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_kwargs)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load file containing pickled data when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "# Change this path to your own\n",
    "data_path = 'Data/'\n",
    "\n",
    "train_input_path = data_path + 'train_input.npy'\n",
    "train_target_path = data_path + 'train_target.npy'\n",
    "val_input_path = data_path + 'val_input.npy'\n",
    "val_target_path = data_path + 'val_target.npy'\n",
    "\n",
    "data.input_train = data.load_npy_file(train_input_path)\n",
    "data.target_train = data.load_npy_file(train_target_path)\n",
    "data.input_val = data.load_npy_file(val_input_path)\n",
    "data.target_val = data.load_npy_file(val_target_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08fefd3",
   "metadata": {},
   "source": [
    "### Huggingface URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4949018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "month = '0001-05'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bdab6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://huggingface.co/datasets/LEAP/ClimSim_low-res/blob/main/train/0001-02/E3SM-MMF.mli.0001-02-01-00000.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16486d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "url2 = \"https://huggingface.co/datasets/LEAP/ClimSim_low-res/raw/main/train/0001-02/E3SM-MMF.mli.0001-02-01-00000.nc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a91f2b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from netCDF4 import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50e07ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "re = requests.get(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b775118e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'version https://git-lfs.github.com/spec/v1\\noid sha256:bd9389d9dfb9c3f34fa8234bf6bd39ff063ebb855dec3eb6662eb9b92fed8133\\nsize 1897632\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59c962d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "syntax error, unexpected WORD_WORD, expecting SCAN_ATTR or SCAN_DATASET or SCAN_ERROR\n",
      "context: Entry^ not found\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno -90] NetCDF: file not found: 'https://huggingface.co/datasets/LEAP/ClimSim_low-res/raw/main/train/0001-02/E3SM-MMF.mli.0001-02-01-00000.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2464\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4.Dataset.__init__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32msrc/netCDF4/_netCDF4.pyx:2027\u001b[0m, in \u001b[0;36mnetCDF4._netCDF4._ensure_nc_success\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno -90] NetCDF: file not found: 'https://huggingface.co/datasets/LEAP/ClimSim_low-res/raw/main/train/0001-02/E3SM-MMF.mli.0001-02-01-00000.nc'"
     ]
    }
   ],
   "source": [
    "Dataset(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0eb514b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy', 'pynio']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/xarray/backends/api.py:547\u001b[0m, in \u001b[0;36mopen_dataset\u001b[0;34m(filename_or_obj, engine, chunks, cache, decode_cf, mask_and_scale, decode_times, decode_timedelta, use_cftime, concat_characters, decode_coords, drop_variables, inline_array, chunked_array_type, from_array_kwargs, backend_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate(backend_kwargs)\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 547\u001b[0m     engine \u001b[38;5;241m=\u001b[39m \u001b[43mplugins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mguess_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename_or_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_array_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    550\u001b[0m     from_array_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/anaconda3/envs/leap/lib/python3.11/site-packages/xarray/backends/plugins.py:197\u001b[0m, in \u001b[0;36mguess_engine\u001b[0;34m(store_spec)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    190\u001b[0m     error_msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound the following matches with the input file in xarray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms IO \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackends: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompatible_engines\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. But their dependencies may not be installed, see:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    193\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/user-guide/io.html \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m     )\n\u001b[0;32m--> 197\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(error_msg)\n",
      "\u001b[0;31mValueError\u001b[0m: did not find a match in any of xarray's currently installed IO backends ['netcdf4', 'scipy', 'pynio']. Consider explicitly selecting one of the installed engines via the ``engine`` parameter, or installing additional IO dependencies, see:\nhttps://docs.xarray.dev/en/stable/getting-started-guide/installing.html\nhttps://docs.xarray.dev/en/stable/user-guide/io.html"
     ]
    }
   ],
   "source": [
    "xr.open_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d3407",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74e81461",
   "metadata": {},
   "source": [
    "### Train OLS Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848b773b",
   "metadata": {},
   "outputs": [],
   "source": [
    "const_model = data.target_train.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77bf89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.input_train\n",
    "bias_vector = np.ones((X.shape[0], 1))\n",
    "X = np.concatenate((X, bias_vector), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03717056",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlr_weights = np.linalg.inv(X.transpose()@X)@X.transpose()@data.target_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d373f68",
   "metadata": {},
   "source": [
    "### Train Own Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb08816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cd6616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f4e826",
   "metadata": {},
   "source": [
    "## Evals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2baa899",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.set_pressure_grid(data_split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82894f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constant Prediction\n",
    "const_pred_val = np.repeat(const_model[np.newaxis, :], data.target_val.shape[0], axis = 0)\n",
    "print(const_pred_val.shape)\n",
    "\n",
    "# Multiple Linear Regression\n",
    "X_val = data.input_val\n",
    "bias_vector_val = np.ones((X_val.shape[0], 1))\n",
    "X_val = np.concatenate((X_val, bias_vector_val), axis=1)\n",
    "mlr_pred_val = X_val@mlr_weights\n",
    "print(mlr_pred_val.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b075c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your prediction here\n",
    "\n",
    "# Load predictions into data_utils object\n",
    "data.model_names = ['const', 'mlr'] # add names of your models here\n",
    "preds = [const_pred_val, mlr_pred_val] # add your custom predictions here\n",
    "data.preds_val = dict(zip(data.model_names, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee2940c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2aadae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.reweight_target(data_split = 'val')\n",
    "data.reweight_preds(data_split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539a05d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.metrics_names = ['MAE', 'RMSE', 'R2', 'bias']\n",
    "data.create_metrics_df(data_split = 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857023e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca114cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50fac4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a94fcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set plotting settings\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "letters = string.ascii_lowercase\n",
    "\n",
    "# create custom dictionary for plotting\n",
    "dict_var = data.metrics_var_val\n",
    "plot_df_byvar = {}\n",
    "for metric in data.metrics_names:\n",
    "    plot_df_byvar[metric] = pd.DataFrame([dict_var[model][metric] for model in data.model_names],\n",
    "                                               index=data.model_names)\n",
    "    plot_df_byvar[metric] = plot_df_byvar[metric].rename(columns = data.var_short_names).transpose()\n",
    "\n",
    "# plot figure\n",
    "fig, axes = plt.subplots(nrows  = len(data.metrics_names), sharex = True)\n",
    "for i in range(len(data.metrics_names)):\n",
    "    plot_df_byvar[data.metrics_names[i]].plot.bar(\n",
    "        legend = False,\n",
    "        ax = axes[i])\n",
    "    if data.metrics_names[i] != 'R2':\n",
    "        axes[i].set_ylabel('$W/m^2$')\n",
    "    else:\n",
    "        axes[i].set_ylim(0,1)\n",
    "\n",
    "    axes[i].set_title(f'({letters[i]}) {data.metrics_names[i]}')\n",
    "axes[i].set_xlabel('Output variable')\n",
    "axes[i].set_xticklabels(plot_df_byvar[data.metrics_names[i]].index, \\\n",
    "    rotation=0, ha='center')\n",
    "\n",
    "axes[0].legend(columnspacing = .9, \n",
    "               labelspacing = .3,\n",
    "               handleheight = .07,\n",
    "               handlelength = 1.5,\n",
    "               handletextpad = .2,\n",
    "               borderpad = .2,\n",
    "               ncol = 3,\n",
    "               loc = 'upper right')\n",
    "fig.set_size_inches(7,8)\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
